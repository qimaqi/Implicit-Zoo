{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Zoo Data visualization \n",
    "A quick start notebook for performing INRs visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not installed already, please setup all dependencies.\n",
    "! git clone https://github.com/qimaqi/Implicit-Zoo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Implicit-Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data samples\n",
    "We can download some data demo samples and place it it dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/scl/fi/7poo46l7dgvfk6mmuk1s4/demo_data.zip?rlkey=lxbbk20vn45pjego266h0aaz1&st=iodkjet4&dl=0 -O demo_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip demo_data.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np \n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create INRs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_coordinates(\n",
    "    shape: Union[Tuple[int], List[int]],\n",
    "    bs: int,\n",
    "    coord_range: Union[Tuple[int], List[int]] = (-1, 1),\n",
    ") -> torch.Tensor:\n",
    "    y_coordinates = np.linspace(coord_range[0], coord_range[1], shape[0])\n",
    "    x_coordinates = np.linspace(coord_range[0], coord_range[1], shape[1])\n",
    "    x_coordinates, y_coordinates = np.meshgrid(x_coordinates, y_coordinates)\n",
    "    x_coordinates = x_coordinates.flatten()\n",
    "    y_coordinates = y_coordinates.flatten()\n",
    "    coordinates = np.stack([x_coordinates, y_coordinates]).T\n",
    "    coordinates = np.repeat(coordinates[np.newaxis, ...], bs, axis=0)\n",
    "    return torch.from_numpy(coordinates).type(torch.float)\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=1.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        w0=30.0,\n",
    "        c=6.0,\n",
    "        is_first=False,\n",
    "        use_bias=True,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "        self.c = c\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.is_first = is_first\n",
    "\n",
    "        weight = torch.zeros(dim_out, dim_in)\n",
    "        bias = torch.zeros(dim_out) if use_bias else None\n",
    "        self.init_(weight, bias, c=c, w0=w0)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = nn.Parameter(bias) if use_bias else None\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def init_(self, weight: torch.Tensor, bias: torch.Tensor, c: float, w0: float):\n",
    "        dim = self.dim_in\n",
    "\n",
    "        w_std = (1 / dim) if self.is_first else (math.sqrt(c / dim) / w0)\n",
    "        weight.uniform_(-w_std, w_std)\n",
    "\n",
    "        if bias is not None:\n",
    "            # bias.uniform_(-w_std, w_std)\n",
    "            bias.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = F.linear(x, self.weight, self.bias)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class INR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_dim: int = 2,\n",
    "        n_layers: int = 3,\n",
    "        up_scale: int = 16,\n",
    "        out_channels: int = 1,\n",
    "        device='cpu',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = np.array(image_size)\n",
    "        self.device = device\n",
    "        hidden_dim = in_dim * up_scale\n",
    "        self.layers = [Siren(dim_in=in_dim, dim_out=hidden_dim)]\n",
    "        for i in range(n_layers - 2):\n",
    "            self.layers.append(Siren(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, out_channels))\n",
    "        self.seq = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.seq(x) \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_entire_image(self):\n",
    "        input = make_coordinates(self.image_size, 1).to(self.device)\n",
    "        image = self.forward(input)\n",
    "        image = image.view(*self.image_size, -1)\n",
    "        image = image.permute(2, 0, 1).detach().cpu()\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Cifar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren_model = INR(\n",
    "    image_size=(32,32),\n",
    "    in_dim=2,\n",
    "    n_layers=3,\n",
    "    up_scale=32,\n",
    "    out_channels=3,\n",
    ")\n",
    "# load a checkpoint from dataset\n",
    "path_to_checkpoint = 'dataset/demo_data/cifar_demo.ckpt'\n",
    "siren_model.load_state_dict(torch.load(path_to_checkpoint, map_location='cpu')['params'])\n",
    "image = siren_model.predict_entire_image()\n",
    "\n",
    "# save image for visualization\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "inv_normalize = transforms.Compose([    \n",
    "    transforms.Normalize(mean = torch.zeros_like(mean),\n",
    "                        std = 1/std),\n",
    "    transforms.Normalize(mean = -mean,\n",
    "                        std = torch.ones_like(std)),\n",
    "                        ])\n",
    "                        \n",
    "image = inv_normalize(image)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10,10)) \n",
    "axs.imshow(pil_image)\n",
    "axs.set_title('Recunstracted Images from INRs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ImageNet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren_model = INR(\n",
    "    image_size=(256,256),\n",
    "    in_dim=2,\n",
    "    n_layers=4,\n",
    "    up_scale=128,\n",
    "    out_channels=3,\n",
    ")\n",
    "# load a checkpoint from dataset\n",
    "path_to_checkpoint = 'dataset/demo_data/imagenet_demo.ckpt'\n",
    "siren_model.load_state_dict(torch.load(path_to_checkpoint, map_location='cpu')['params'])\n",
    "image = siren_model.predict_entire_image()\n",
    "                        \n",
    "image = inv_normalize(image)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10,10)) \n",
    "axs.imshow(pil_image)\n",
    "axs.set_title('Recunstracted Images from INRs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Cityscapes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siren_model = INR(\n",
    "    image_size=(320,640),\n",
    "    in_dim=2,\n",
    "    n_layers=5,\n",
    "    up_scale=128,\n",
    "    out_channels=3,\n",
    ")\n",
    "# load a checkpoint from dataset\n",
    "path_to_checkpoint = 'dataset/demo_data/cityscapes_demo.ckpt'\n",
    "siren_model.load_state_dict(torch.load(path_to_checkpoint, map_location='cpu')['params'])\n",
    "image = siren_model.predict_entire_image()\n",
    "\n",
    "image = inv_normalize(image)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "pil_image = transforms.ToPILImage()(image)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10,20)) \n",
    "axs.imshow(pil_image)\n",
    "axs.set_title('Recunstracted Images from INRs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
